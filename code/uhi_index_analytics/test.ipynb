{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db09b6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5.5)\n",
      "Requirement already satisfied: pandas in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: seaborn in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (0.13.2)\n",
      "Requirement already satisfied: matplotlib in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.1)\n",
      "Requirement already satisfied: geopandas in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.0.1)\n",
      "Requirement already satisfied: shapely in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: rasterio in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (1.4.3)\n",
      "Requirement already satisfied: rioxarray in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.18.2)\n",
      "Requirement already satisfied: pyproj in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.7.1)\n",
      "Requirement already satisfied: affine in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (2.4.0)\n",
      "Requirement already satisfied: apache-sedona in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (1.7.1)\n",
      "Requirement already satisfied: PyYAML in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (6.0.2)\n",
      "Requirement already satisfied: ijson in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (3.3.0)\n",
      "Requirement already satisfied: tqdm in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (4.67.1)\n",
      "Requirement already satisfied: cdsapi in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (0.7.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from pyspark->-r requirements.txt (line 1)) (0.10.9.7)\n",
      "Requirement already satisfied: numpy>=1.23.2 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from geopandas->-r requirements.txt (line 5)) (0.10.0)\n",
      "Requirement already satisfied: attrs in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from rasterio->-r requirements.txt (line 7)) (25.3.0)\n",
      "Requirement already satisfied: certifi in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from rasterio->-r requirements.txt (line 7)) (2025.1.31)\n",
      "Requirement already satisfied: click>=4.0 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from rasterio->-r requirements.txt (line 7)) (8.1.8)\n",
      "Requirement already satisfied: cligj>=0.5 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from rasterio->-r requirements.txt (line 7)) (0.7.2)\n",
      "Requirement already satisfied: click-plugins in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from rasterio->-r requirements.txt (line 7)) (1.1.1)\n",
      "Requirement already satisfied: xarray>=2024.7.0 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from rioxarray->-r requirements.txt (line 8)) (2025.3.1)\n",
      "Requirement already satisfied: colorama in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from tqdm->-r requirements.txt (line 14)) (0.4.6)\n",
      "Requirement already satisfied: datapi in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from cdsapi->-r requirements.txt (line 15)) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.5.0 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from cdsapi->-r requirements.txt (line 15)) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from requests>=2.5.0->cdsapi->-r requirements.txt (line 15)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from requests>=2.5.0->cdsapi->-r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from requests>=2.5.0->cdsapi->-r requirements.txt (line 15)) (2.4.0)\n",
      "Requirement already satisfied: multiurl>=0.3.2 in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from datapi->cdsapi->-r requirements.txt (line 15)) (0.3.5)\n",
      "Requirement already satisfied: typing-extensions in d:\\hw_project\\bigdata\\code\\uhi_index_analytics\\.venv\\lib\\site-packages (from datapi->cdsapi->-r requirements.txt (line 15)) (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ed44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import final\n",
    "from numpy import shape\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, to_timestamp, expr, split, when, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType\n",
    "from sedona.spark import SedonaContext\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.core.SpatialRDD import PointRDD\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.core.enums import FileDataSplitter\n",
    "from sedona.core.geom.envelope import Envelope\n",
    "from sedona.sql.types import GeometryType\n",
    "import yaml\n",
    "import os\n",
    "from shapely import wkt\n",
    "from shapely.geometry import box\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "from pyspark.sql.functions import explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d887bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_PREFIX = \"hdfs://26.3.217.119:9000\"\n",
    "READ_DIR = f\"{HDFS_PREFIX}/climate_data/uhi_index_analytics/raw/\"\n",
    "SAVE_DIR = f\"{HDFS_PREFIX}/climate_data/uhi_index_analytics/preprocessed/\"\n",
    "\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "COORDS = config[\"coords\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "139d297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    # Create a Sedona Context using individual config calls\n",
    "    builder = SedonaContext.builder()\n",
    "\n",
    "    # Set application name\n",
    "    builder = builder.config(\"spark.app.name\", \"GeoSpatialPreprocessing\")\n",
    "\n",
    "    # Add each configuration individually\n",
    "    builder = builder.config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,org.datasyslab:geotools-wrapper:1.4.0-28.2\",\n",
    "    )\n",
    "    builder = builder.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    builder = builder.config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\")\n",
    "    builder = builder.config(\"spark.sql.catalog.sedona\", \"org.apache.sedona.sql.SpatialCatalog\")\n",
    "    builder = builder.config(\"spark.sql.catalog.sedona.options\", \"{}\")\n",
    "    builder = builder.config(\"spark.driver.memory\", \"8g\")\n",
    "\n",
    "    # Create and return the Sedona context\n",
    "    sedona = builder.getOrCreate()\n",
    "\n",
    "    return sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa60db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583a3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_street_spark(spark, config, readfile, savefile):\n",
    "    # Define coordinates from config\n",
    "    COORDS = config[\"coords\"]\n",
    "\n",
    "    # Read GeoJSON file\n",
    "    df = spark.read.format(\"json\").load(readfile)\n",
    "\n",
    "    print(\"Input schema:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # Register temporary view\n",
    "    df.createOrReplaceTempView(\"streets_raw\")\n",
    "\n",
    "    # Initial selection of needed columns and cleanup\n",
    "    df = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            OBJECTID, SegmentID, Join_ID, StreetCode, Street, \n",
    "            TrafDir, StreetWidth_Min, StreetWidth_Max, \n",
    "            trim(RW_TYPE) as RW_TYPE, POSTED_SPEED,\n",
    "            Number_Travel_Lanes, Number_Park_Lanes, Number_Total_Lanes,\n",
    "            FeatureTyp, SegmentTyp, BikeLane, BIKE_TRAFDIR,\n",
    "            XFrom, YFrom, XTo, YTo, ArcCenterX, ArcCenterY,\n",
    "            NodeIDFrom, NodeIDTo, NodeLevelF, NodeLevelT,\n",
    "            TRUCK_ROUTE_TYPE, Shape__Length, geometry\n",
    "        FROM streets_raw\n",
    "        WHERE \n",
    "            geometry IS NOT NULL\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Apply filters using Spark SQL\n",
    "    df = df.filter(\n",
    "        ~col(\"FeatureTyp\").isin(\"2\", \"5\", \"7\", \"9\", \"F\")\n",
    "        & ~col(\"SegmentTyp\").isin(\"G\", \"F\")\n",
    "        & ~col(\"RW_TYPE\").isin(\"4\", \"12\", \"14\")\n",
    "        & (col(\"Status\") == \"2\")\n",
    "    )\n",
    "\n",
    "    # Spatial filter using Sedona\n",
    "    df.createOrReplaceTempView(\"streets_filtered\")\n",
    "\n",
    "    bbox_query = f\"\"\"\n",
    "    SELECT * FROM streets_filtered\n",
    "    WHERE ST_Contains(\n",
    "        ST_GeomFromWKT('POLYGON(({COORDS[0]} {COORDS[1]}, {COORDS[2]} {COORDS[1]}, \n",
    "                              {COORDS[2]} {COORDS[3]}, {COORDS[0]} {COORDS[3]}, \n",
    "                              {COORDS[0]} {COORDS[1]}))'),\n",
    "        ST_GeomFromGeoJSON(to_json(geometry))\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark.sql(bbox_query)\n",
    "\n",
    "    # Configure replication and save\n",
    "    spark.conf.set(\"spark.hadoop.dfs.replication\", \"1\")\n",
    "\n",
    "    # # Save to parquet for efficiency\n",
    "    # df.write.format(\"parquet\").mode(\"overwrite\").save(f\"{savefile}.parquet\")\n",
    "\n",
    "    # # Optionally save as GeoJSON (less efficient but matches original)\n",
    "    # df.write.format(\"json\").mode(\"overwrite\").save(f\"{savefile}\")\n",
    "\n",
    "    # print(f\"Data is saved at {savefile}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7245ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_building(spark, config, readfile, savefile):\n",
    "    COORDS = config[\"coords\"]\n",
    "\n",
    "    # Use multiline option for JSON arrays\n",
    "    raw = spark.read.format(\"json\").option(\"multiline\", \"true\").load(readfile)\n",
    "\n",
    "    print(\"Input schema:\")\n",
    "    raw.printSchema()\n",
    "\n",
    "    raw.createOrReplaceTempView(\"buildings_raw\")\n",
    "\n",
    "    raw = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            bin, cnstrct_yr, heightroof, the_geom as geometry, base_bbl, mpluto_bbl,\n",
    "            TO_TIMESTAMP(lstmoddate) as lstmoddate,\n",
    "            feat_code, lststatype\n",
    "        FROM buildings_raw\n",
    "        WHERE \n",
    "            the_geom IS NOT NULL AND \n",
    "            bin IS NOT NULL AND \n",
    "            cnstrct_yr IS NOT NULL AND \n",
    "            heightroof IS NOT NULL\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Convert data types\n",
    "    raw = (\n",
    "        raw.withColumn(\"bin\", col(\"bin\").cast(IntegerType()))\n",
    "        .withColumn(\"cnstrct_yr\", col(\"cnstrct_yr\").cast(IntegerType()))\n",
    "        .withColumn(\"heightroof\", col(\"heightroof\").cast(FloatType()))\n",
    "        .withColumn(\"feat_code\", col(\"feat_code\").cast(IntegerType()))\n",
    "        .withColumn(\"base_bbl\", col(\"base_bbl\").cast(StringType()))\n",
    "        .withColumn(\"mpluto_bbl\", col(\"mpluto_bbl\").cast(StringType()))\n",
    "    )\n",
    "\n",
    "    filtered = raw.filter(\n",
    "        (col(\"cnstrct_yr\") <= 2021)\n",
    "        & (col(\"bin\") / 1000000).cast(IntegerType()).isin(1, 2)\n",
    "        & (col(\"heightroof\") >= 12)\n",
    "        & (col(\"feat_code\").isin(1006, 2100))\n",
    "        & (col(\"lstmoddate\") < \"2021-07-24\")\n",
    "        & (col(\"lststatype\") == \"Constructed\")\n",
    "    )\n",
    "\n",
    "    filtered.createOrReplaceTempView(\"buildings_filtered\")\n",
    "\n",
    "    # Convert the_geom GeoJSON to WKT format for spatial operations\n",
    "    bbox_query = f\"\"\"\n",
    "    SELECT * FROM buildings_filtered\n",
    "    WHERE ST_Contains(\n",
    "        ST_GeomFromWKT('POLYGON(({COORDS[0]} {COORDS[1]}, {COORDS[2]} {COORDS[1]}, \n",
    "                              {COORDS[2]} {COORDS[3]}, {COORDS[0]} {COORDS[3]}, \n",
    "                              {COORDS[0]} {COORDS[1]}))'),\n",
    "        ST_GeomFromGeoJSON(geometry)\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    bbox = spark.sql(bbox_query)\n",
    "\n",
    "    bbox.createOrReplaceTempView(\"buildings_bbox\")\n",
    "\n",
    "    area_query = \"\"\"\n",
    "        SELECT *, \n",
    "               ST_Area(ST_Transform(ST_GeomFromGeoJSON(geometry), 'EPSG:4326', 'EPSG:2263')) as shape_area\n",
    "        FROM buildings_bbox\n",
    "        \"\"\"\n",
    "\n",
    "    shape_area = spark.sql(area_query)\n",
    "\n",
    "    shape_area.createOrReplaceTempView(\"buildings_shape_area\")\n",
    "    \n",
    "    print(shape_area.count())\n",
    "\n",
    "    filter_shape = shape_area.filter(col(\"shape_area\") >= 400)\n",
    "\n",
    "    filter_shape.createOrReplaceTempView(\"buildings_filter_shape\")\n",
    "\n",
    "    final = spark.sql(\n",
    "        \"\"\"\n",
    "        SELECT bin, cnstrct_yr, heightroof, geometry, base_bbl, mpluto_bbl,\n",
    "               lstmoddate, feat_code, lststatype, shape_area\n",
    "        FROM buildings_filter_shape\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    spark.conf.set(\"spark.hadoop.dfs.replication\", \"1\")\n",
    "    final.write.format(\"parquet\").mode(\"overwrite\").save(f\"{savefile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832de71c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_spark_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m spark = \u001b[43mcreate_spark_session\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'create_spark_session' is not defined"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fc718621",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").option(\"multiline\", \"true\").load(f\"{READ_DIR}nyco.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "550adeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input schema:\n",
      "root\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- geometry: struct (nullable = true)\n",
      " |    |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- properties: struct (nullable = true)\n",
      " |    |    |    |-- OBJECTID: long (nullable = true)\n",
      " |    |    |    |-- OVERLAY: string (nullable = true)\n",
      " |    |    |    |-- Shape__Area: double (nullable = true)\n",
      " |    |    |    |-- Shape__Length: double (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Input schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2c625f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"features\" in df.columns:\n",
    "    zoning_df = df.select(explode(\"features\").alias(\"feature\"))\n",
    "    zoning_df = zoning_df.select(\n",
    "        col(\"feature.id\").alias(\"id\"),\n",
    "        col(\"feature.geometry\").alias(\"geometry\"),\n",
    "        col(\"feature.properties.*\")  # Flatten properties\n",
    "    )\n",
    "else:\n",
    "    # If already at feature level\n",
    "    zoning_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "54782bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = zoning_df.withColumn(\"Shape__Area\", col(\"Shape__Area\").cast(FloatType()))\n",
    "select_df = zoning_df.withColumn(\"Shape__Length\", col(\"Shape__Length\").cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c403bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = select_df.filter(col(\"OBJECTID\") != 944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ef951480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9637"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "99908869",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df.createOrReplaceTempView(\"zoning_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a6427783",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_query = f\"\"\"\n",
    "    SELECT * FROM zoning_filtered\n",
    "    WHERE ST_Contains(\n",
    "        ST_GeomFromWKT('POLYGON(({COORDS[0]} {COORDS[1]}, {COORDS[2]} {COORDS[1]}, \n",
    "                              {COORDS[2]} {COORDS[3]}, {COORDS[0]} {COORDS[3]}, \n",
    "                              {COORDS[0]} {COORDS[1]}))'),\n",
    "        ST_GeomFromGeoJSON(to_json(geometry))\n",
    "    )\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "76c4bd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_df = spark.sql(bbox_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "31652a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_df.createOrReplaceTempView(\"temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c36e7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT * FROM temp_view \n",
    "    WHERE OBJECTID != '944'\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "085ff416",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1398.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 99.0 failed 1 times, most recent failure: Lost task 0.0 in stage 99.0 (TID 92) (host.docker.internal executor driver): java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:31)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:20)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:16)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeoJSON(FormatUtils.java:188)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeometry(FormatUtils.java:286)\r\n\tat org.apache.sedona.common.Constructors.geomFromText(Constructors.java:112)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromGeoJSON.eval(Constructors.scala:160)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Predicate.eval(Predicates.scala:52)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.And_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1(FilterEvaluatorFactory.scala:42)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1$adapted(FilterEvaluatorFactory.scala:41)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1915)\r\n\tat com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1355)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:311)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers.handleNonArray(PrimitiveArrayDeserializers.java:219)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:890)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:864)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1409)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:220)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:170)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:136)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1296)\r\n\tat com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:74)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4801)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2974)\r\n\tat org.wololo.geojson.GeoJSONFactory.readGeometry(GeoJSONFactory.java:69)\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:28)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.GeneratedMethodAccessor164.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:31)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:20)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:16)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeoJSON(FormatUtils.java:188)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeometry(FormatUtils.java:286)\r\n\tat org.apache.sedona.common.Constructors.geomFromText(Constructors.java:112)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromGeoJSON.eval(Constructors.scala:160)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Predicate.eval(Predicates.scala:52)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.And_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1(FilterEvaluatorFactory.scala:42)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1$adapted(FilterEvaluatorFactory.scala:41)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1915)\r\n\tat com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1355)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:311)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers.handleNonArray(PrimitiveArrayDeserializers.java:219)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:890)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:864)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1409)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:220)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:170)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:136)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1296)\r\n\tat com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:74)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4801)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2974)\r\n\tat org.wololo.geojson.GeoJSONFactory.readGeometry(GeoJSONFactory.java:69)\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:28)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[210]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfiltered_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m604\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:978\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    971\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    972\u001b[39m         message_parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m    975\u001b[39m         },\n\u001b[32m    976\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1398.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 99.0 failed 1 times, most recent failure: Lost task 0.0 in stage 99.0 (TID 92) (host.docker.internal executor driver): java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:31)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:20)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:16)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeoJSON(FormatUtils.java:188)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeometry(FormatUtils.java:286)\r\n\tat org.apache.sedona.common.Constructors.geomFromText(Constructors.java:112)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromGeoJSON.eval(Constructors.scala:160)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Predicate.eval(Predicates.scala:52)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.And_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1(FilterEvaluatorFactory.scala:42)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1$adapted(FilterEvaluatorFactory.scala:41)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1915)\r\n\tat com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1355)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:311)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers.handleNonArray(PrimitiveArrayDeserializers.java:219)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:890)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:864)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1409)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:220)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:170)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:136)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1296)\r\n\tat com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:74)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4801)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2974)\r\n\tat org.wololo.geojson.GeoJSONFactory.readGeometry(GeoJSONFactory.java:69)\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:28)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.GeneratedMethodAccessor164.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:31)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:20)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:16)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeoJSON(FormatUtils.java:188)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeometry(FormatUtils.java:286)\r\n\tat org.apache.sedona.common.Constructors.geomFromText(Constructors.java:112)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromGeoJSON.eval(Constructors.scala:160)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Predicate.eval(Predicates.scala:52)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.And_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1(FilterEvaluatorFactory.scala:42)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1$adapted(FilterEvaluatorFactory.scala:41)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1915)\r\n\tat com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1355)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:311)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers.handleNonArray(PrimitiveArrayDeserializers.java:219)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:890)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:864)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1409)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:220)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:170)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:136)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1296)\r\n\tat com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:74)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4801)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2974)\r\n\tat org.wololo.geojson.GeoJSONFactory.readGeometry(GeoJSONFactory.java:69)\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:28)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show(604, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cbf14e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- OBJECTID: long (nullable = true)\n",
      " |-- Shape__Area: double (nullable = true)\n",
      " |-- Shape__Length: float (nullable = true)\n",
      " |-- ZONEDIST: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spatial_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "97cf17a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1336.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 78.0 failed 1 times, most recent failure: Lost task 0.0 in stage 78.0 (TID 74) (host.docker.internal executor driver): java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:31)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:20)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:16)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeoJSON(FormatUtils.java:188)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeometry(FormatUtils.java:286)\r\n\tat org.apache.sedona.common.Constructors.geomFromText(Constructors.java:112)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromGeoJSON.eval(Constructors.scala:160)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Predicate.eval(Predicates.scala:52)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1(FilterEvaluatorFactory.scala:42)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1$adapted(FilterEvaluatorFactory.scala:41)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1915)\r\n\tat com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1355)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:311)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers.handleNonArray(PrimitiveArrayDeserializers.java:219)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:890)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:864)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1409)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:220)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:170)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:136)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1296)\r\n\tat com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:74)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4801)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2974)\r\n\tat org.wololo.geojson.GeoJSONFactory.readGeometry(GeoJSONFactory.java:69)\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:28)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:31)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:20)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:16)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeoJSON(FormatUtils.java:188)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeometry(FormatUtils.java:286)\r\n\tat org.apache.sedona.common.Constructors.geomFromText(Constructors.java:112)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromGeoJSON.eval(Constructors.scala:160)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Predicate.eval(Predicates.scala:52)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1(FilterEvaluatorFactory.scala:42)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1$adapted(FilterEvaluatorFactory.scala:41)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1915)\r\n\tat com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1355)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:311)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers.handleNonArray(PrimitiveArrayDeserializers.java:219)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:890)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:864)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1409)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:220)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:170)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:136)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1296)\r\n\tat com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:74)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4801)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2974)\r\n\tat org.wololo.geojson.GeoJSONFactory.readGeometry(GeoJSONFactory.java:69)\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:28)\r\n\t... 30 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[170]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspatial_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1240\u001b[39m, in \u001b[36mDataFrame.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1217\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   1218\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[32m   1219\u001b[39m \n\u001b[32m   1220\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1238\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   1239\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o1336.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 78.0 failed 1 times, most recent failure: Lost task 0.0 in stage 78.0 (TID 74) (host.docker.internal executor driver): java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:31)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:20)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:16)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeoJSON(FormatUtils.java:188)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeometry(FormatUtils.java:286)\r\n\tat org.apache.sedona.common.Constructors.geomFromText(Constructors.java:112)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromGeoJSON.eval(Constructors.scala:160)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Predicate.eval(Predicates.scala:52)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1(FilterEvaluatorFactory.scala:42)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1$adapted(FilterEvaluatorFactory.scala:41)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1915)\r\n\tat com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1355)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:311)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers.handleNonArray(PrimitiveArrayDeserializers.java:219)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:890)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:864)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1409)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:220)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:170)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:136)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1296)\r\n\tat com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:74)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4801)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2974)\r\n\tat org.wololo.geojson.GeoJSONFactory.readGeometry(GeoJSONFactory.java:69)\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:28)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.lang.RuntimeException: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:31)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:20)\r\n\tat org.wololo.jts2geojson.GeoJSONReader.read(GeoJSONReader.java:16)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeoJSON(FormatUtils.java:188)\r\n\tat org.apache.sedona.common.utils.FormatUtils.readGeometry(FormatUtils.java:286)\r\n\tat org.apache.sedona.common.Constructors.geomFromText(Constructors.java:112)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromGeoJSON.eval(Constructors.scala:160)\r\n\tat org.apache.spark.sql.sedona_sql.expressions.ST_Predicate.eval(Predicates.scala:52)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1(FilterEvaluatorFactory.scala:42)\r\n\tat org.apache.spark.sql.execution.FilterEvaluatorFactory$FilterPartitionEvaluator.$anonfun$eval$1$adapted(FilterEvaluatorFactory.scala:41)\r\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:515)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `double[]`: no String-argument constructor/factory method to deserialize from String value ('[-73.9103449368792,40.8230547655496]')\n at [Source: UNKNOWN; byte offset: #UNKNOWN] (through reference chain: org.wololo.geojson.MultiPolygon[\"coordinates\"]->java.lang.Object[][0]->java.lang.Object[][0]->java.lang.Object[][0])\r\n\tat com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.reportBadDefinition(DeserializationContext.java:1915)\r\n\tat com.fasterxml.jackson.databind.DatabindContext.reportBadDefinition(DatabindContext.java:414)\r\n\tat com.fasterxml.jackson.databind.DeserializationContext.handleMissingInstantiator(DeserializationContext.java:1355)\r\n\tat com.fasterxml.jackson.databind.deser.std.StdDeserializer._deserializeFromString(StdDeserializer.java:311)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers.handleNonArray(PrimitiveArrayDeserializers.java:219)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:890)\r\n\tat com.fasterxml.jackson.databind.deser.std.PrimitiveArrayDeserializers$DoubleDeser.deserialize(PrimitiveArrayDeserializers.java:864)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:216)\r\n\tat com.fasterxml.jackson.databind.deser.std.ObjectArrayDeserializer.deserialize(ObjectArrayDeserializer.java:26)\r\n\tat com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:545)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:570)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:439)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1409)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:352)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:220)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:187)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:170)\r\n\tat com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:136)\r\n\tat com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeWithType(BeanDeserializerBase.java:1296)\r\n\tat com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:74)\r\n\tat com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:323)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4801)\r\n\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2974)\r\n\tat org.wololo.geojson.GeoJSONFactory.readGeometry(GeoJSONFactory.java:69)\r\n\tat org.wololo.geojson.GeoJSONFactory.create(GeoJSONFactory.java:28)\r\n\t... 30 more\r\n"
     ]
    }
   ],
   "source": [
    "spatial_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c96b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_query = \"\"\"\n",
    "    SELECT *, \n",
    "           ST_Area(ST_Transform(ST_GeomFromGeoJSON(to_json(geometry)), 'EPSG:4326', 'EPSG:2263')) as area_sqm\n",
    "    FROM zoning_bbox\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b961267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "area_df = spark.sql(area_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba0a03ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------+--------------------+-------------------+\n",
      "|OBJECTID|Shape__Area|Shape__Length|            geometry|           area_sqm|\n",
      "+--------+-----------+-------------+--------------------+-------------------+\n",
      "|     298|  142598.66|    1665.3289|{[[[-73.892051298...|1.381991657409128E7|\n",
      "|     303|   780.5681|    322.51852|{[[[-73.903506374...|  75756.17098496873|\n",
      "|     304|   75266.35|    1259.1455|{[[[-73.891167366...|  7294919.686124862|\n",
      "|     305|  38180.176|    1243.3271|{[[[-73.890457615...| 3699470.9591754447|\n",
      "|     306|  15974.725|    598.66406|{[[[-73.899581025...| 1550268.6037419448|\n",
      "+--------+-----------+-------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "area_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3bbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d3a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"features\" in df.columns:\n",
    "    streets_df = df.select(explode(\"features\").alias(\"feature\"))\n",
    "    streets_df = streets_df.select(\n",
    "        col(\"feature.id\").alias(\"id\"),\n",
    "        col(\"feature.geometry\").alias(\"geometry\"),\n",
    "        col(\"feature.properties.*\")  # Flatten properties\n",
    "    )\n",
    "else:\n",
    "    # If already at feature level\n",
    "    streets_df = df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register temporary view\n",
    "streets_df.createOrReplaceTempView(\"streets_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e81cad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial filtering based on the street criteria\n",
    "filtered_df = streets_df.filter(\n",
    "    (~col(\"FeatureTyp\").isin(\"2\", \"5\", \"7\", \"9\", \"F\")) &\n",
    "    (~col(\"SegmentTyp\").isin(\"G\", \"F\")) &\n",
    "    (~col(\"RW_TYPE\").isin(\"4\", \"12\", \"14\")) &\n",
    "    (col(\"Status\") == \"2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eba5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = [\n",
    "    \"OBJECTID\",\n",
    "    \"SegmentID\",\n",
    "    \"Join_ID\",\n",
    "    \"StreetCode\",\n",
    "    \"Street\",\n",
    "    \"TrafDir\",\n",
    "    \"StreetWidth_Min\",\n",
    "    \"StreetWidth_Max\",\n",
    "    \"RW_TYPE\",\n",
    "    \"POSTED_SPEED\",\n",
    "    \"Number_Travel_Lanes\",\n",
    "    \"Number_Park_Lanes\",\n",
    "    \"Number_Total_Lanes\",\n",
    "    \"FeatureTyp\",\n",
    "    \"SegmentTyp\",\n",
    "    \"BikeLane\",\n",
    "    \"BIKE_TRAFDIR\",\n",
    "    \"XFrom\",\n",
    "    \"YFrom\",\n",
    "    \"XTo\",\n",
    "    \"YTo\",\n",
    "    \"ArcCenterX\",\n",
    "    \"ArcCenterY\",\n",
    "    \"NodeIDFrom\",\n",
    "    \"NodeIDTo\",\n",
    "    \"NodeLevelF\",\n",
    "    \"NodeLevelT\",\n",
    "    \"TRUCK_ROUTE_TYPE\",\n",
    "    \"Shape__Length\",\n",
    "    \"geometry\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53b6838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_df = filtered_df.select([col(c) for c in cols_to_keep if c in filtered_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8af9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string fields to proper types\n",
    "select_df = select_df.withColumn(\"StreetWidth_Min\", col(\"StreetWidth_Min\").cast(IntegerType()))\n",
    "select_df = select_df.withColumn(\"StreetWidth_Max\", col(\"StreetWidth_Max\").cast(IntegerType()))\n",
    "select_df = select_df.withColumn(\"Shape__Length\", col(\"Shape__Length\").cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cebbf163",
   "metadata": {},
   "outputs": [],
   "source": [
    " select_df.createOrReplaceTempView(\"streets_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffc0a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8165a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_query = f\"\"\"\n",
    "SELECT * FROM streets_filtered\n",
    "WHERE ST_Contains(\n",
    "    ST_GeomFromWKT('POLYGON(({COORDS[0]} {COORDS[1]}, {COORDS[2]} {COORDS[1]}, \n",
    "                            {COORDS[2]} {COORDS[3]}, {COORDS[0]} {COORDS[3]}, \n",
    "                            {COORDS[0]} {COORDS[1]}))'),\n",
    "    ST_GeomFromGeoJSON(to_json(geometry))\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "750722a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_df = spark.sql(bbox_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e2a40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------------+----------+---------------+-------+---------------+---------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+-------+------+-------+------+----------+----------+----------+--------+----------+----------+----------------+-------------+--------------------+\n",
      "|OBJECTID|SegmentID|        Join_ID|StreetCode|         Street|TrafDir|StreetWidth_Min|StreetWidth_Max|RW_TYPE|POSTED_SPEED|Number_Travel_Lanes|Number_Park_Lanes|Number_Total_Lanes|FeatureTyp|SegmentTyp|BikeLane|BIKE_TRAFDIR|  XFrom| YFrom|    XTo|   YTo|ArcCenterX|ArcCenterY|NodeIDFrom|NodeIDTo|NodeLevelF|NodeLevelT|TRUCK_ROUTE_TYPE|Shape__Length|            geometry|\n",
      "+--------+---------+---------------+----------+---------------+-------+---------------+---------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+-------+------+-------+------+----------+----------+----------+--------+----------+----------+----------------+-------------+--------------------+\n",
      "|       1|  0078126|  2251001000000|    226700|EAST 168 STREET|      T|             34|             34|      1|          25|                  2|                2|                 4|         0|         U|        |            |1010964|241812|1011265|241555|         0|         0|   0047740| 9045677|         M|         M|                |    396.03094|{[[-73.9034682815...|\n",
      "|       2|  0079796|  2798401000000|    274810|WEST 192 STREET|      A|             30|             30|      1|          25|                  1|                2|                 3|         0|         U|        |            |1011577|255024|1011335|255164|         0|         0|   0048679| 0048678|         M|         M|                |     279.3605|{[[-73.9012021014...|\n",
      "|       3|  0077356|  2728001000000|    270420|   UNION AVENUE|      W|             34|             34|      1|          25|                  1|                2|                 3|         0|         U|        |            |1011601|239640|1011786|240230|         0|         0|   0047288| 0047822|         M|         M|                |    618.32715|{[[-73.9011781129...|\n",
      "|       4|  0077356|21279502000000X|    270420|   UNION AVENUE|      W|             34|             34|      1|          25|                  1|                2|                 3|         0|         U|        |            |1011601|239640|1011786|240230|         0|         0|   0047288| 0047822|         M|         M|                |    618.32715|{[[-73.9011781129...|\n",
      "|       5|  0077356|21279503000000X|    270420|   UNION AVENUE|      W|             34|             34|      1|          25|                  1|                2|                 3|         0|         U|        |            |1011601|239640|1011786|240230|         0|         0|   0047288| 0047822|         M|         M|                |    618.32715|{[[-73.9011781129...|\n",
      "+--------+---------+---------------+----------+---------------+-------+---------------+---------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+-------+------+-------+------+----------+----------+----------+--------+----------+----------+----------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spatial_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62eb22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_df.createOrReplaceTempView(\"streets_bbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f3d6b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_query = \"\"\"\n",
    "    SELECT *, \n",
    "           ST_Length(ST_Transform(ST_GeomFromGeoJSON(to_json(geometry)), 'EPSG:2263', 'EPSG:4326')) as length_meters\n",
    "    FROM streets_bbox\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae632afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = spark.sql(length_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f32ad9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------------+----------+---------------+-------+---------------+---------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+-------+------+-------+------+----------+----------+----------+--------+----------+----------+----------------+-------------+--------------------+------------------+\n",
      "|OBJECTID|SegmentID|        Join_ID|StreetCode|         Street|TrafDir|StreetWidth_Min|StreetWidth_Max|RW_TYPE|POSTED_SPEED|Number_Travel_Lanes|Number_Park_Lanes|Number_Total_Lanes|FeatureTyp|SegmentTyp|BikeLane|BIKE_TRAFDIR|  XFrom| YFrom|    XTo|   YTo|ArcCenterX|ArcCenterY|NodeIDFrom|NodeIDTo|NodeLevelF|NodeLevelT|TRUCK_ROUTE_TYPE|Shape__Length|            geometry|     length_meters|\n",
      "+--------+---------+---------------+----------+---------------+-------+---------------+---------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+-------+------+-------+------+----------+----------+----------+--------+----------+----------+----------------+-------------+--------------------+------------------+\n",
      "|       1|  0078126|  2251001000000|    226700|EAST 168 STREET|      T|             34|             34|      1|          25|                  2|                2|                 4|         0|         U|        |            |1010964|241812|1011265|241555|         0|         0|   0047740| 9045677|         M|         M|                |    396.03094|{[[-73.9034682815...|6558.0776888721675|\n",
      "|       2|  0079796|  2798401000000|    274810|WEST 192 STREET|      A|             30|             30|      1|          25|                  1|                2|                 3|         0|         U|        |            |1011577|255024|1011335|255164|         0|         0|   0048679| 0048678|         M|         M|                |     279.3605|{[[-73.9012021014...| 5220.472357394877|\n",
      "|       3|  0077356|  2728001000000|    270420|   UNION AVENUE|      W|             34|             34|      1|          25|                  1|                2|                 3|         0|         U|        |            |1011601|239640|1011786|240230|         0|         0|   0047288| 0047822|         M|         M|                |    618.32715|{[[-73.9011781129...| 4795.128485720481|\n",
      "|       4|  0077356|21279502000000X|    270420|   UNION AVENUE|      W|             34|             34|      1|          25|                  1|                2|                 3|         0|         U|        |            |1011601|239640|1011786|240230|         0|         0|   0047288| 0047822|         M|         M|                |    618.32715|{[[-73.9011781129...| 4795.128485720481|\n",
      "|       5|  0077356|21279503000000X|    270420|   UNION AVENUE|      W|             34|             34|      1|          25|                  1|                2|                 3|         0|         U|        |            |1011601|239640|1011786|240230|         0|         0|   0047288| 0047822|         M|         M|                |    618.32715|{[[-73.9011781129...| 4795.128485720481|\n",
      "+--------+---------+---------------+----------+---------------+-------+---------------+---------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+-------+------+-------+------+----------+----------+----------+--------+----------+----------+----------------+-------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "length_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d6458ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39838"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42cf0cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input schema:\n",
      "root\n",
      " |-- base_bbl: string (nullable = true)\n",
      " |-- bin: string (nullable = true)\n",
      " |-- cnstrct_yr: string (nullable = true)\n",
      " |-- doitt_id: string (nullable = true)\n",
      " |-- feat_code: string (nullable = true)\n",
      " |-- geomsource: string (nullable = true)\n",
      " |-- globalid: string (nullable = true)\n",
      " |-- groundelev: string (nullable = true)\n",
      " |-- heightroof: string (nullable = true)\n",
      " |-- lstmoddate: string (nullable = true)\n",
      " |-- lststatype: string (nullable = true)\n",
      " |-- mpluto_bbl: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- shape_area: string (nullable = true)\n",
      " |-- shape_len: string (nullable = true)\n",
      " |-- the_geom: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      "\n",
      "60738\n"
     ]
    }
   ],
   "source": [
    "filter_building(spark, config, f\"{READ_DIR}/building/building.json\", \"building.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b87b202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------+--------+----------+----------+---------+----------+----------+\n",
      "|bin|cnstrct_yr|heightroof|geometry|base_bbl|mpluto_bbl|lstmoddate|feat_code|lststatype|shape_area|\n",
      "+---+----------+----------+--------+--------+----------+----------+---------+----------+----------+\n",
      "+---+----------+----------+--------+--------+----------+----------+---------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"building.parquet\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d4f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_building(spark, readfile, savefile):\n",
    "    \"\"\"Extract building features using PySpark\"\"\"\n",
    "    # Read parquet file\n",
    "    df = spark.read.parquet(f\"{READ_DIR}{readfile}\")\n",
    "\n",
    "    # Column types already set in parquet, but ensure consistency\n",
    "    df = df.select(\n",
    "        col(\"bin\").cast(IntegerType()),\n",
    "        col(\"cnstrct_yr\").cast(IntegerType()),\n",
    "        col(\"heightroof\").cast(FloatType()),\n",
    "        col(\"shape_area\").cast(FloatType()),\n",
    "        col(\"geometry\"),\n",
    "        col(\"base_bbl\").cast(StringType()),\n",
    "        col(\"mpluto_bbl\").cast(StringType()),\n",
    "    )\n",
    "\n",
    "    # Save as GeoJSON\n",
    "    df.write.format(\"parquet\").mode(\"overwrite\").save(f\"{SAVE_DIR}{savefile}\")\n",
    "    print(f\"Data is saved at {SAVE_DIR}{savefile}.\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "686f04e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Missing/incomplete configuration file: C:\\Users\\Leo/.cdsapirc",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m      8\u001b[39m dataset = \u001b[33m\"\u001b[39m\u001b[33msatellite-sea-level-global\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m request = {\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvariable\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mmonthly_mean\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvdt2024\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m client = \u001b[43mcdsapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m client.retrieve(dataset, request).download()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\cdsapi\\api.py:281\u001b[39m, in \u001b[36mClient.__new__\u001b[39m\u001b[34m(cls, url, key, *args, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, url=\u001b[38;5;28;01mNone\u001b[39;00m, key=\u001b[38;5;28;01mNone\u001b[39;00m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     _, token, _ = \u001b[43mget_url_key_verify\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m token:\n\u001b[32m    283\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\cdsapi\\api.py:69\u001b[39m, in \u001b[36mget_url_key_verify\u001b[39m\u001b[34m(url, key, verify)\u001b[39m\n\u001b[32m     66\u001b[39m             verify = \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mint\u001b[39m(config.get(\u001b[33m\"\u001b[39m\u001b[33mverify\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)))\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMissing/incomplete configuration file: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % (dotrc))\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# If verify is still None, then we set to default value of True\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mException\u001b[39m: Missing/incomplete configuration file: C:\\Users\\Leo/.cdsapirc"
     ]
    }
   ],
   "source": [
    "import cdsapi\n",
    "import yaml\n",
    "\n",
    "# with open(\"config.yaml\", \"r\") as file:\n",
    "#     config = yaml.safe_load(file)\n",
    "\n",
    "# Initialize the CDS API client\n",
    "dataset = \"satellite-sea-level-global\"\n",
    "request = {\n",
    "    \"variable\": [\"monthly_mean\"],\n",
    "    \"year\": [\n",
    "        \"1993\",\n",
    "        \"1994\",\n",
    "        \"1995\",\n",
    "        \"1996\",\n",
    "        \"1997\",\n",
    "        \"1998\",\n",
    "        \"1999\",\n",
    "        \"2000\",\n",
    "        \"2001\",\n",
    "        \"2002\",\n",
    "        \"2003\",\n",
    "        \"2004\",\n",
    "        \"2005\",\n",
    "        \"2006\",\n",
    "        \"2007\",\n",
    "        \"2008\",\n",
    "        \"2009\",\n",
    "        \"2010\",\n",
    "        \"2011\",\n",
    "        \"2012\",\n",
    "        \"2013\",\n",
    "        \"2014\",\n",
    "        \"2015\",\n",
    "        \"2016\",\n",
    "        \"2017\",\n",
    "        \"2018\",\n",
    "        \"2019\",\n",
    "        \"2020\",\n",
    "        \"2021\",\n",
    "        \"2022\",\n",
    "        \"2023\",\n",
    "    ],\n",
    "    \"month\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"],\n",
    "    \"version\": \"vdt2024\",\n",
    "}\n",
    "\n",
    "client = cdsapi.Client()\n",
    "client.retrieve(dataset, request).download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94459c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, when, expr, lit, degrees, atan2, split, substring\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, ArrayType, StructType, StructField\n",
    "from sedona.spark import SedonaContext\n",
    "from sedona.sql.types import GeometryType\n",
    "import yaml\n",
    "import math\n",
    "import os\n",
    "from pyspark.sql.functions import trim\n",
    "from pyproj import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2799c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335ebddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(\n",
    "    core: int = 6,\n",
    "    driver_menory: str = \"8g\",\n",
    "):\n",
    "    # Create a Sedona Context using individual config calls\n",
    "    builder = SedonaContext.builder()\n",
    "\n",
    "    # Set application name\n",
    "    builder = builder.config(\"spark.app.name\", \"GeoSpatialPreprocessing\")\n",
    "\n",
    "    # Add each configuration individually\n",
    "    builder = builder.config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.1,org.datasyslab:geotools-wrapper:1.4.0-28.2\",\n",
    "    )\n",
    "    builder = builder.master(f\"local[{core}]\")\n",
    "    builder = builder.config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    builder = builder.config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\")\n",
    "    builder = builder.config(\"spark.sql.catalog.sedona\", \"org.apache.sedona.sql.SpatialCatalog\")\n",
    "    builder = builder.config(\"spark.sql.catalog.sedona.options\", \"{}\")\n",
    "    builder = builder.config(\"spark.driver.memory\", f\"{driver_menory}\")\n",
    "\n",
    "    # Create and return the Sedona context\n",
    "    sedona = builder.getOrCreate()\n",
    "\n",
    "    return sedona\n",
    "\n",
    "\n",
    "def convert_to_float_spark(df, column):\n",
    "    return df.withColumn(column, expr(f\"CAST(NULLIF(TRIM({column}), '') AS FLOAT)\"))\n",
    "\n",
    "\n",
    "def feet_to_degree_spark(df, lon_col, lat_col):\n",
    "    transformer = Transformer.from_crs(\"EPSG:2263\", \"EPSG:4326\", always_xy=True)\n",
    "\n",
    "    def transform_coords(lon, lat):\n",
    "        if lon is not None and lat is not None:\n",
    "            return transformer.transform(lon, lat)\n",
    "        return (None, None)\n",
    "\n",
    "    transform_udf = udf(\n",
    "        transform_coords, StructType([StructField(\"lon\", FloatType()), StructField(\"lat\", FloatType())])\n",
    "    )\n",
    "    return df.withColumn(\"transformed\", transform_udf(col(lon_col), col(lat_col)))\n",
    "\n",
    "\n",
    "def street_direction_spark(df):\n",
    "    df = df.withColumn(\"dx\", col(\"XTo\") - col(\"XFrom\"))\n",
    "    df = df.withColumn(\"dy\", col(\"YTo\") - col(\"YFrom\"))\n",
    "    df = df.withColumn(\"angle\", expr(\"degrees(atan2(dy, dx))\"))\n",
    "    df = df.withColumn(\"angle\", when(col(\"angle\") < 0, col(\"angle\") + 360).otherwise(col(\"angle\")))\n",
    "    df = df.withColumn(\n",
    "        \"direction\",\n",
    "        when(\n",
    "            ((22.5 <= col(\"angle\")) & (col(\"angle\") < 67.5)) | ((157.5 <= col(\"angle\")) & (col(\"angle\") < 202.5)),\n",
    "            \"NE-SW\",\n",
    "        )\n",
    "        .when(\n",
    "            ((67.5 <= col(\"angle\")) & (col(\"angle\") < 112.5)) | ((247.5 <= col(\"angle\")) & (col(\"angle\") < 292.5)),\n",
    "            \"N-S\",\n",
    "        )\n",
    "        .when(\n",
    "            ((112.5 <= col(\"angle\")) & (col(\"angle\") < 157.5)) | ((292.5 <= col(\"angle\")) & (col(\"angle\") < 337.5)),\n",
    "            \"NW-SE\",\n",
    "        )\n",
    "        .otherwise(\"E-W\"),\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_feature_street_spark(spark, readfile, savefile):\n",
    "    # Read GeoJSON file\n",
    "    df = spark.read.parquet(readfile)\n",
    "    print(\"Input schema:\")\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Convert string to float\n",
    "    columns_to_convert = [\n",
    "        \"RW_TYPE\",\n",
    "        \"Number_Travel_Lanes\",\n",
    "        \"Number_Park_Lanes\",\n",
    "        \"Number_Total_Lanes\",\n",
    "        \"POSTED_SPEED\",\n",
    "        \"BikeLane\",\n",
    "        \"TRUCK_ROUTE_TYPE\",\n",
    "    ]\n",
    "    for col_name in columns_to_convert:\n",
    "        df = convert_to_float_spark(df, col_name)\n",
    "    df = df.withColumn(\"RW_TYPE\", col(\"RW_TYPE\").cast(\"int\"))\n",
    "\n",
    "    # Handle BIKE_TRAFDIR\n",
    "    df = df.withColumn(\n",
    "        \"BIKE_TRAFDIR\", when(expr(\"TRIM(BIKE_TRAFDIR) = ''\"), None).otherwise(expr(\"TRIM(BIKE_TRAFDIR)\"))\n",
    "    )\n",
    "\n",
    "    # Calculate average street width\n",
    "    df = df.withColumn(\"street_width_avg\", (col(\"StreetWidth_Min\") + col(\"StreetWidth_Max\")) / 2)\n",
    "\n",
    "    # Transform coordinates\n",
    "    df = feet_to_degree_spark(df, \"XFrom\", \"YFrom\")\n",
    "    df = df.withColumn(\"XFrom\", col(\"transformed.lon\")).withColumn(\"YFrom\", col(\"transformed.lat\")).drop(\"transformed\")\n",
    "    df = feet_to_degree_spark(df, \"XTo\", \"YTo\")\n",
    "    df = df.withColumn(\"XTo\", col(\"transformed.lon\")).withColumn(\"YTo\", col(\"transformed.lat\")).drop(\"transformed\")\n",
    "\n",
    "    # Calculate street direction\n",
    "    df = street_direction_spark(df)\n",
    "\n",
    "    # Extract features\n",
    "    df = df.select(\n",
    "        [\n",
    "            \"OBJECTID\",\n",
    "            \"Join_ID\",\n",
    "            \"StreetCode\",\n",
    "            \"Street\",\n",
    "            \"TrafDir\",\n",
    "            \"StreetWidth_Min\",\n",
    "            \"StreetWidth_Max\",\n",
    "            \"street_width_avg\",\n",
    "            \"RW_TYPE\",\n",
    "            \"POSTED_SPEED\",\n",
    "            \"Number_Travel_Lanes\",\n",
    "            \"Number_Park_Lanes\",\n",
    "            \"Number_Total_Lanes\",\n",
    "            \"FeatureTyp\",\n",
    "            \"SegmentTyp\",\n",
    "            \"BikeLane\",\n",
    "            \"BIKE_TRAFDIR\",\n",
    "            \"TRUCK_ROUTE_TYPE\",\n",
    "            \"Shape__Length\",\n",
    "            \"direction\",\n",
    "            \"geometry\",\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # # Save to Parquet\n",
    "    # df.write.format(\"parquet\").mode(\"overwrite\").save(savefile)\n",
    "    # print(f\"Data is saved at {savefile}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34cbabff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['VIRTUAL_ENV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1bd9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PYSPARK_PYTHON\"] = os.path.join(os.environ[\"VIRTUAL_ENV\"], \"Scripts\", \"python.exe\")\n",
    "spark = create_spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c8f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HDFS_PREFIX = \"hdfs://26.3.217.119:9000\"\n",
    "READ_DIR = f\"{HDFS_PREFIX}/climate_data/uhi_index_analytics/preprocessed/\"\n",
    "SAVE_DIR = f\"{HDFS_PREFIX}/climate_data/uhi_index_analytics/features_extraction/\"\n",
    "\n",
    "extract_feature_street_spark(spark, f\"{READ_DIR}street.parquet\", \"street_features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5f186c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+----------+-----------------+-------+---------------+---------------+----------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+----------------+-------------+---------+--------------------------------------------------------------------------------------------+\n",
      "|OBJECTID|Join_ID        |StreetCode|Street           |TrafDir|StreetWidth_Min|StreetWidth_Max|street_width_avg|RW_TYPE|POSTED_SPEED|Number_Travel_Lanes|Number_Park_Lanes|Number_Total_Lanes|FeatureTyp|SegmentTyp|BikeLane|BIKE_TRAFDIR|TRUCK_ROUTE_TYPE|Shape__Length|direction|geometry                                                                                    |\n",
      "+--------+---------------+----------+-----------------+-------+---------------+---------------+----------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+----------------+-------------+---------+--------------------------------------------------------------------------------------------+\n",
      "|1       |2251001000000  |226700    |EAST 168 STREET  |T      |34             |34             |34.0            |1      |25.0        |2.0                |2.0              |4.0               |0         |U         |NULL    |NULL        |NULL            |396.03094    |NW-SE    |{[[-73.9034682815071, 40.8303620794707], [-73.9023814757185, 40.8296549103045]], LineString}|\n",
      "|2       |2798401000000  |274810    |WEST 192 STREET  |A      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |279.3605     |NW-SE    |{[[-73.901202101452, 40.8666213684178], [-73.9020747884371, 40.8670073957739]], LineString} |\n",
      "|3       |2728001000000  |270420    |UNION AVENUE     |W      |34             |34             |34.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |618.32715    |NE-SW    |{[[-73.9011781129234, 40.8243971910923], [-73.9005068263005, 40.8260159042596]], LineString}|\n",
      "|4       |21279502000000X|270420    |UNION AVENUE     |W      |34             |34             |34.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |618.32715    |NE-SW    |{[[-73.9011781129234, 40.8243971910923], [-73.9005068263005, 40.8260159042596]], LineString}|\n",
      "|5       |21279503000000X|270420    |UNION AVENUE     |W      |34             |34             |34.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |618.32715    |NE-SW    |{[[-73.9011781129234, 40.8243971910923], [-73.9005068263005, 40.8260159042596]], LineString}|\n",
      "|6       |21279504000000X|270420    |UNION AVENUE     |W      |34             |34             |34.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |618.32715    |NE-SW    |{[[-73.9011781129234, 40.8243971910923], [-73.9005068263005, 40.8260159042596]], LineString}|\n",
      "|9       |2147601000000  |219720    |CLAREMONT PARKWAY|T      |70             |70             |70.0            |1      |25.0        |4.0                |2.0              |6.0               |0         |U         |NULL    |NULL        |NULL            |252.18307    |E-W      |{[[-73.9053850046466, 40.8397772683225], [-73.9045463781778, 40.8395062490999]], LineString}|\n",
      "|10      |2718001000000  |269220    |TOPPING AVENUE   |W      |28             |30             |29.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |291.75702    |N-S      |{[[-73.9055885394703, 40.8466972761627], [-73.9055230613734, 40.8474965182373]], LineString}|\n",
      "|11      |2255001000000  |226910    |EAST 175 STREET  |T      |34             |35             |34.5            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |249.53316    |E-W      |{[[-73.9055885394703, 40.8466972761627], [-73.9046885853694, 40.8466521508809]], LineString}|\n",
      "|13      |2182001000000  |223520    |DAVIDSON AVENUE  |W      |34             |35             |34.5            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |2.0     |FT          |NULL            |259.55362    |NE-SW    |{[[-73.9052653178272, 40.8580554661155], [-73.9048698972397, 40.8587015132331]], LineString}|\n",
      "|14      |2334801000000  |234220    |FULTON AVENUE    |W      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |979.3721     |NE-SW    |{[[-73.9034364375375, 40.8322246640605], [-73.9018574812373, 40.8346304086362]], LineString}|\n",
      "|15      |20124101000000N|234220    |FULTON AVENUE    |W      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |979.3721     |NE-SW    |{[[-73.9034364375375, 40.8322246640605], [-73.9018574812373, 40.8346304086362]], LineString}|\n",
      "|16      |20124501000000N|234220    |FULTON AVENUE    |W      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |979.3721     |NE-SW    |{[[-73.9034364375375, 40.8322246640605], [-73.9018574812373, 40.8346304086362]], LineString}|\n",
      "|17      |20124401000000N|234220    |FULTON AVENUE    |W      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |979.3721     |NE-SW    |{[[-73.9034364375375, 40.8322246640605], [-73.9018574812373, 40.8346304086362]], LineString}|\n",
      "|18      |21216502000000X|234220    |FULTON AVENUE    |W      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |979.3721     |NE-SW    |{[[-73.9034364375375, 40.8322246640605], [-73.9018574812373, 40.8346304086362]], LineString}|\n",
      "|19      |21216501000000G|234220    |FULTON AVENUE    |W      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |979.3721     |NE-SW    |{[[-73.9034364375375, 40.8322246640605], [-73.9018574812373, 40.8346304086362]], LineString}|\n",
      "|20      |20124201000000N|234220    |FULTON AVENUE    |W      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |NULL    |NULL        |NULL            |979.3721     |NE-SW    |{[[-73.9034364375375, 40.8322246640605], [-73.9018574812373, 40.8346304086362]], LineString}|\n",
      "|22      |2253501000000  |226850    |EAST 173 STREET  |A      |30             |30             |30.0            |1      |25.0        |1.0                |2.0              |3.0               |0         |U         |2.0     |TF          |NULL            |289.71674    |E-W      |{[[-73.9011194192623, 40.8416070237177], [-73.900156429642, 40.8412948122544]], LineString} |\n",
      "|23      |2410001000000  |242820    |JEROME AVENUE    |T      |68             |70             |69.0            |1      |25.0        |4.0                |2.0              |6.0               |0         |U         |NULL    |NULL        |2.0             |868.0147     |NE-SW    |{[[-73.9073343226406, 40.8538247019289], [-73.9055714979726, 40.855795610312]], LineString} |\n",
      "|24      |2352001000000  |236420    |GRAND CONCOURSE  |T      |52             |54             |53.0            |1      |25.0        |4.0                |NULL             |4.0               |0         |B         |NULL    |NULL        |NULL            |336.70645    |NE-SW    |{[[-73.9075326780194, 40.8478244489309], [-73.9067824163853, 40.8485521098019]], LineString}|\n",
      "+--------+---------------+----------+-----------------+-------+---------------+---------------+----------------+-------+------------+-------------------+-----------------+------------------+----------+----------+--------+------------+----------------+-------------+---------+--------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"hdfs://26.3.217.119:9000/climate_data/uhi_index_analytics/features_extraction/street_features.parquet\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8268df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+-------+--------------+-----+-----+-----+\n",
      "|year|month|decimal_date|average|deseasonalized|ndays|sdev |unc  |\n",
      "+----+-----+------------+-------+--------------+-----+-----+-----+\n",
      "|1958|3    |1958.2027   |315.71 |314.44        |-1   |-9.99|-0.99|\n",
      "|1958|4    |1958.2877   |317.45 |315.16        |-1   |-9.99|-0.99|\n",
      "|1958|5    |1958.3699   |317.51 |314.69        |-1   |-9.99|-0.99|\n",
      "|1958|6    |1958.4548   |317.27 |315.15        |-1   |-9.99|-0.99|\n",
      "|1958|7    |1958.537    |315.87 |315.2         |-1   |-9.99|-0.99|\n",
      "|1958|8    |1958.6219   |314.93 |316.21        |-1   |-9.99|-0.99|\n",
      "|1958|9    |1958.7068   |313.21 |316.11        |-1   |-9.99|-0.99|\n",
      "|1958|10   |1958.789    |312.42 |315.41        |-1   |-9.99|-0.99|\n",
      "|1958|11   |1958.874    |313.33 |315.21        |-1   |-9.99|-0.99|\n",
      "|1958|12   |1958.9562   |314.67 |315.43        |-1   |-9.99|-0.99|\n",
      "|1959|1    |1959.0411   |315.58 |315.52        |-1   |-9.99|-0.99|\n",
      "|1959|2    |1959.126    |316.49 |315.84        |-1   |-9.99|-0.99|\n",
      "|1959|3    |1959.2027   |316.65 |315.37        |-1   |-9.99|-0.99|\n",
      "|1959|4    |1959.2877   |317.72 |315.42        |-1   |-9.99|-0.99|\n",
      "|1959|5    |1959.3699   |318.29 |315.46        |-1   |-9.99|-0.99|\n",
      "|1959|6    |1959.4548   |318.15 |316.0         |-1   |-9.99|-0.99|\n",
      "|1959|7    |1959.537    |316.54 |315.87        |-1   |-9.99|-0.99|\n",
      "|1959|8    |1959.6219   |314.8  |316.09        |-1   |-9.99|-0.99|\n",
      "|1959|9    |1959.7068   |313.84 |316.75        |-1   |-9.99|-0.99|\n",
      "|1959|10   |1959.789    |313.33 |316.34        |-1   |-9.99|-0.99|\n",
      "+----+-----+------------+-------+--------------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(f\"hdfs://26.3.217.119:9000/climate_data/trend_analytics/preprocessed/clean_co2.parquet\").show(\n",
    "    truncate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83d16a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d917fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mDefaultCredentialsError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m bucket \u001b[38;5;129;01min\u001b[39;00m buckets:\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mprint\u001b[39m(bucket.name)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mlist_buckets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mlist_buckets\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_buckets\u001b[39m():\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Lists all buckets.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     storage_client = \u001b[43mstorage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     buckets = storage_client.list_buckets()\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m bucket \u001b[38;5;129;01min\u001b[39;00m buckets:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\google\\cloud\\storage\\client.py:247\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, project, credentials, _http, client_info, client_options, use_auth_w_custom_endpoint, extra_headers, api_key)\u001b[39m\n\u001b[32m    244\u001b[39m             no_project = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    245\u001b[39m             project = \u001b[33m\"\u001b[39m\u001b[33m<none>\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_http\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_http\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Validate that the universe domain of the credentials matches the\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# universe domain of the client.\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._credentials.universe_domain != \u001b[38;5;28mself\u001b[39m.universe_domain:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\google\\cloud\\client\\__init__.py:338\u001b[39m, in \u001b[36mClientWithProject.__init__\u001b[39m\u001b[34m(self, project, credentials, client_options, _http)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, project=\u001b[38;5;28;01mNone\u001b[39;00m, credentials=\u001b[38;5;28;01mNone\u001b[39;00m, client_options=\u001b[38;5;28;01mNone\u001b[39;00m, _http=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[43m_ClientProjectMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     Client.\u001b[34m__init__\u001b[39m(\n\u001b[32m    340\u001b[39m         \u001b[38;5;28mself\u001b[39m, credentials=credentials, client_options=client_options, _http=_http\n\u001b[32m    341\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\google\\cloud\\client\\__init__.py:286\u001b[39m, in \u001b[36m_ClientProjectMixin.__init__\u001b[39m\u001b[34m(self, project, credentials)\u001b[39m\n\u001b[32m    283\u001b[39m     project = \u001b[38;5;28mgetattr\u001b[39m(credentials, \u001b[33m\"\u001b[39m\u001b[33mproject_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     project = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_determine_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    290\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mProject was not passed and could not be \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    291\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdetermined from the environment.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    292\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\google\\cloud\\client\\__init__.py:305\u001b[39m, in \u001b[36m_ClientProjectMixin._determine_default\u001b[39m\u001b[34m(project)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_determine_default\u001b[39m(project):\n\u001b[32m    304\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Helper:  use default project detection.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_determine_default_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\google\\cloud\\_helpers\\__init__.py:152\u001b[39m, in \u001b[36m_determine_default_project\u001b[39m\u001b[34m(project)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Determine default project ID explicitly or implicitly as fall-back.\u001b[39;00m\n\u001b[32m    141\u001b[39m \n\u001b[32m    142\u001b[39m \u001b[33;03mSee :func:`google.auth.default` for details on how the default project\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    149\u001b[39m \u001b[33;03m:returns: Default project if it can be determined.\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m project \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     _, project = \u001b[43mgoogle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m project\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HW_Project\\BigData\\code\\uhi_index_analytics\\.venv\\Lib\\site-packages\\google\\auth\\_default.py:685\u001b[39m, in \u001b[36mdefault\u001b[39m\u001b[34m(scopes, request, quota_project_id, default_scopes)\u001b[39m\n\u001b[32m    677\u001b[39m             _LOGGER.warning(\n\u001b[32m    678\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mNo project ID could be determined. Consider running \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    679\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    680\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33menvironment variable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    681\u001b[39m                 environment_vars.PROJECT,\n\u001b[32m    682\u001b[39m             )\n\u001b[32m    683\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[31mDefaultCredentialsError\u001b[39m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "def list_buckets():\n",
    "    \"\"\"Lists all buckets.\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    buckets = storage_client.list_buckets()\n",
    "\n",
    "    for bucket in buckets:\n",
    "        print(bucket.name)\n",
    "\n",
    "list_buckets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85d76499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f7ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 3 4 5]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
